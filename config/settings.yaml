 # Note: etl/etl_map.py will use DATABASE_URL or POSTGRES_* from the environment if set.
# This db_url acts as a fallback.
db_url: postgresql://postgres:postgres@localhost:5432/iob_knowledge

ifc:
  schema_version: IFC4.3
  # Path to Samples file you added
  json_path: ./Samples/ifc_classes_with_psets.json
  # Use parent-chain context (class + ancestors) for IFC text when embedding/matching
  use_parent_context: true

uniclass:
  revision: 2025-09
  autodetect_revision: true
  enforce_monotonic_revision: true
  # Prefer this: auto-discover all .xlsx in folder
  xlsx_dir: ./Samples/uniclassTables
  # Or, alternatively, provide CSVs per facet (leave commented if not used)
  # csv_paths:
  #   EF: C:/path/to/Uniclass/EF.csv
  #   Ss: C:/path/to/Uniclass/Ss.csv
  #   Pr: C:/path/to/Uniclass/Pr.csv
  #   # add more facets as needed: TE, PM, Ac, En, SL, Ro, Co

output:
  dir: ./output
  viewer_json: ./output/viewer_mapping.json

matching:
  top_k: 10
  auto_accept_threshold: 0.85
  review_threshold: 0.55
  # weight for embedding similarity in final score (0..1). 0 disables blending
  embedding_weight: 0.15
  # number of nearest neighbors to consider from embeddings per IFC item
  embedding_top_k: 25
  direction: uniclass_to_ifc

  anchor_bonus: 0.3
  anchor_use_ancestors: true
  # Discipline gating to reduce cross-domain mismatches (e.g., ELEC vs cleaning)
  discipline_filter: soft   # options: none | soft (penalize) | hard (exclude)
  discipline_penalty: 0.4   # multiplier if soft-mismatch
  # Source of labels used for gating: heuristic | llm | llm_then_heuristic
  discipline_source: llm_then_heuristic
  # Convenience: list facets to skip entirely during matching
  # Focus only on PR facet; skip everything else
  # skip_tables: [AC, CO, EF, EN, FI, MA, PC, PM, RK, RO, SL, SS, ZZ, TE]
  # Optional facet-aware rules to constrain candidates
  # - skip: true            -> ignore the entire facet
  # - abstract_only: true   -> allow only abstract IFC classes
  # - require_ifc_roots: [] -> require IFC to be a subclass of any of the listed roots
  rules:
    PR:
      require_ifc_roots: [IfcProduct]
    # SS:
    #   require_ifc_roots: [IfcGroup, IfcSystem]
    # PM:
    #   require_ifc_roots: [IfcProcess]
    # AC:
    #   require_ifc_roots: [IfcProcess]
  # basic synonym hints to boost lexical scoring
  synonyms:
    - ["hvac","mechanical services","building services"]
    - ["curtain wall","glazed facade","curtainwall"]
    - ["pipe","piping"]
    - ["duct","ductwork"]

features:
  enabled: true
  attribute_tokens: []

evaluation:
  baseline_run_id: null
  max_precision_drop: 0.05
  min_recall: 0.70
  accept_threshold: 0.70
  fail_on_regression: false


embedding:
  model: mxbai-embed-large
  endpoint: http://localhost:11434
  batch_size: 16
  timeout_s: 200
  # 1024 for mxbai-embed-large and bge-m3; 768 for jina-embeddings-v2-base-en
  expected_dim: 1024
  # OpenAI-specific embedding model and expected dim when using --openai
  # e.g., text-embedding-3-small (1536) or text-embedding-3-large (3072)
  openai_model: text-embedding-3-small
  openai_expected_dim: 1536
  # pgvector ANN tuning
  ivf_lists: 128
  ivf_probes: 8 # between 5-8 (lower = faster, slightly lower recall)
  # query timeout for embedding neighbor search (milliseconds)
  query_timeout_ms: 5000

rerank:
  top_n: 5
  model: mistral:7b-instruct
  # OpenAI-specific chat model when using --openai (e.g., gpt-4o-mini)
  openai_model: gpt-4o-mini
  endpoint: http://localhost:11434
  temperature: 0.2
  max_tokens: 512
  fewshot_per_facet: 3
  timeout_s: 210

